---
title: "Student Enrollment Forecasting"
date: 2020-01-16
tags: [machine learning, forecasting]
header:
#  image: "/images/enrollmentForecastPic.png"
#excerpt: "Machine Learning, Forecasting"
output: 
  html_notebook:
    code_folding: hide
---
![*Student enrollment forecasting graphic.*](C:/Users/george/Documents/R Assignments/EBB AND FLOW/GeorgeAaronG.github.io/images/enrollmentForecastPic.png)



> **Can we forecast student enrollments using ad-generated data from social media platforms?**

This is the question I tried to answer using a client's digital advertising campaign, spending, and student enrollment data to forecast enrollments for the coming year.  Over Christmas break, I though about how I could use my machine learning training to provide a useful solution to a real world business problem.  As I started to develop KPI charts for Lone Star Medical Career College, a client of a local digitial marketing startup based out of my coworking space, it became apparent to me that this might be my big opportunity to apply the predictive modeling workflow I had just learned in my ML class.  The following notebook highlights my first attempt at this real world problem.

*Note:*  Although each digitial advertising campaign generates hourly data, this was my first attempt at developing forecasting models (or really any kind of real world data model) and so I decided to go with the data supplied on their KPI spreadsheet.  Future development will be a big data project with actual hourly social media advertising data.  Also, the name of the college was changed to ensure company privacy.

```{r message =  FALSE}
# Libraries
library(caret)
library(skimr)
library(corrplot)
```

# The Data

## Imported Data

The predicted variable here is student enrollments.  Why student enrollments?  For a career college, this is the return on investment (ROI).  Technically, the return for all the time and resources that go into digital marketing can be measured in many different ways: demographic exposure, increases in online followers, etc.  In this case, I wanted any work I do to directly reflect the company's bottom line.  This is a common tactic, [the researchers on Garter.com tell me](https://www.gartner.com/en), for any company piloting a data mining initiative.

At first I incorporated other dependent variables into the data such as leads count and social media platform (Google Ads vs. Facebook Ads), but initial model performance seemed rather low.  After some refinement and visual exploration using *Tableau*, I settled for the following dependent variables: campaign type and spending.
```{r}
# Import and clean data
data <- read.csv("C:/Users/george/Documents/Digital Marketing Data/Triggers/spendingAndEnrollments.csv")
dataRGV <- data
dataRGV
```

## Skimmed Data

A quick *skim_to_wide* allows an at-a-glance view of the imported data.  Campaign is a factor variable and spending is numeric.  Before visualizing or processing the campaign variable, I will have to convert it to to a numeric dummy variable for better performance.  I also note that there are only 51 instances of this data, so how I decide to partition the data will have a bigger impact on performance.
```{r}
# Split data
x <- subset(dataRGV, select = -Enrollments)
y <- subset(dataRGV, select = Enrollments)

# Data summary
skimX <- skim_to_wide(x)
skimY <- skim_to_wide(y)
skimX
skimY

# Data partition
seed <- 123
set.seed(seed)
dataPart <- createDataPartition(y$Enrollments,
                              p = .70,
                              list = FALSE)
xTrain <- x[dataPart,]
yTrain <- y[dataPart,]

xTest <- x[-dataPart,]
yTest <- y[-dataPart,]
```

## Training Data Visualization and Correlation

For a better visual component to the training data, I use the standard boxplot and histogram charts.  I also plot the correlation between variables.  No correlation screening seems necessary for this data.
```{r}
# Create numeric set of x variables
xTrainNum <- as.data.frame(sapply(xTrain, as.numeric))

# Boxplot of predictors
boxplot(xTrainNum)
boxplot(yTrain)

# Histograms
hist(xTrainNum$Campaign)
hist(xTrainNum$Spending)
hist(as.numeric(yTrain))

# Correlation plots and correlation ceiling
correlations <- cor(xTrainNum)
corrplot(correlations, method = "number", order = "hclust")
```

# ML Algorithm Training

## Models and Tuned Performance

Of the models I trained, *'rf'* random forest seemed to have the best performance with an RMSE = 4.99 and R^2 = 0.74.  I used dummy variables to account for the Campaign factor variable, and tuned all models.  I preprocessed using the standard *center/scaling*, *YeoJohnson* to normalize the data, and *spatialSign* for outliers (which I don't think is necessary, but I usually add it anyway as it might boost performance a bit).
```{r}
# Model set up
dmy <- dummyVars(" ~ .", data = xTrain, fullRank = T)
xTrainDummy <- data.frame(predict(dmy, newdata = xTrain))

set.seed(seed)
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)


# Predictive modeling w/ built-in feature selection
gridBstLm <- expand.grid(mstop = seq(8, 12, by = 1), nu = 0.5)
set.seed(seed)
modelBstLm <- train(x = xTrainDummy, y = yTrain, 
                method = "BstLm", 
                preProc = c("center", "scale", "YeoJohnson", "spatialSign"),
                tuneGrid = gridBstLm,
                trControl = ctrl)
modelBstLm # RMSE = 5.36

gridEnet <- expand.grid(fraction = seq(0.6, 0.8, by = 0.1), lambda = 0)
set.seed(seed)
modelEnet <- train(x = xTrainDummy, y = yTrain, 
                method = "enet", 
                preProc = c("center", "scale", "YeoJohnson", "spatialSign"),
                tuneGrid = gridEnet,
                trControl = ctrl)
modelEnet # RMSE = 5.15

gridGlmNet <- expand.grid(alpha = c(0.9, 1, 1.1), lambda = c(0.6, 0.7, 0.8))
set.seed(seed)
modelGlmNet <- train(x = xTrainDummy, y = yTrain, 
                method = "glmnet",
                preProc = c("center", "scale", "YeoJohnson", "spatialSign"),
                tuneGrid = gridGlmNet,
                trControl = ctrl)
modelGlmNet # RMSE = 5.17

gridLars <- expand.grid(fraction = c(0.65, 0.7, 0.75))
set.seed(seed)
modelLars <- train(x = xTrainDummy, y = yTrain, 
                method = "lars", 
                preProc = c("center", "scale", "YeoJohnson", "spatialSign"),
                tuneGrid = gridLars,
                trControl = ctrl)
modelLars # RMSE = 5.14

gridRF <- expand.grid(mtry = c(1.5, 2, 2.5))
set.seed(seed)
modelRF <- train(x = xTrainDummy, y = yTrain, 
                method = "rf", 
                preProc = c("center", "scale", "YeoJohnson", "spatialSign"),
                tuneGrid = gridRF,
                trControl = ctrl)
modelRF # RMSE = 4.99, R^2 = 0.74

```

## Model Performance Comparison Visualization
```{r}
# Compare model performances
modelResults <- resamples(list(BstLm = modelBstLm, Enet = modelEnet, GlmNet = modelGlmNet,
                               Lars = modelLars, RF = modelRF))
summary(modelResults)
dotplot(modelResults)

# Check differences
modelDiff <- diff(modelResults)
summary(modelDiff)
dotplot(modelDiff)
```

# Finalized Model Performance on Test Data
```{r}
# Transform test data
dmyTest <- dummyVars(" ~ .", data = xTest, fullRank = T)
xTestDummy <- data.frame(predict(dmyTest, newdata = xTest))

# Evaluate on test data
predictRF <- predict(modelRF, newdata = xTestDummy)
RMSE(predictRF, yTest)
R2(predictRF, yTest)
```

## Final Model Test Details

The final *'rf'* model returned disappointing results for the test data.  Only 49% of the variability can be explained by the model which returned an RMSE = 6.61, a lot higher of an error than with the training set. 
```{r}
# Finalize Model
library(randomForest)
set.seed(seed)
preProcParams <- preProcess(xTrainDummy, method = c("center", "scale", "YeoJohnson", "spatialSign"))
xTrainTrans <- predict(preProcParams, xTrainDummy)
finalModel <- randomForest(x = xTrainTrans, y = yTrain, mstop = 10)
finalModel
summary(finalModel)

# Evaluate on test data
xTestNum <- as.data.frame(sapply(xTest, as.numeric))
set.seed(seed)
xTestTrans <- predict(preProcParams, xTestDummy)
finalModelPredict <- predict(finalModel, newdata = xTestTrans, mstop = 10)

# Calculate performance
finalModelRMSE <- RMSE(finalModelPredict, yTest)
finalModelR2 <- R2(finalModelPredict, yTest)
finalModelRMSE
```

At this point, it seems the model may be **overfit**.  For now, let's proceed to trying an ensemble of the **'rf'** random forest and **'BstLm'** boosted linear models.


## Train and Evaluate 'rf' + 'BstLm' Model Ensemble

The **'rf' + 'BstLm'** model ensemble resulted in a slight decrease in RMSE performance and a slight increase in R^2 performance.  Since overall performance is comparable, I'll go ahead and roll with it for the enrollment forecasting.
```{r}
library(caretEnsemble)

# Train a list of models with caretList()
listOfModels <- c('rf', 'BstLm')
set.seed(seed)
modelList <- caretList(x = xTrainDummy, y = yTrain, 
                    trControl = ctrl,
                    preProc = c("center", "scale", "YeoJohnson", "spatialSign"), 
                    methodList = listOfModels)

# Model results and correlation
results <- resamples(modelList)
summary(results)
dotplot(results)
modelCor(results)

# Combine the models with caretEnsemble()
set.seed(seed)
modelEnsemble <- caretEnsemble(modelList, metric = "RMSE",
                              trControl = trainControl(number = 2))

summary(modelEnsemble) # RMSE = 5.0463

# Evaluate on test data
predictEnsemble <- predict(modelEnsemble, newdata = xTestDummy)
RMSE(predictEnsemble, yTest)
R2(predictEnsemble, yTest)
```

# Final Model Prediction: Forecasted Enrollments

To forecast future student enrollments, I used the following December 2019 spending figures:

* **LVN**: $1,271.86
* **Med Asst**: $2,568.00
* **Pharmacy**: $1,015.64
* **Nurse Aide**: $939.75
* **Med Asst**: $1,688.54

Let's assume the following spending increases by Lone Star Medical Career College:

* Dec + 10%
* Dec + 15%
* Dec + 20%

Based on the different campaign categories and these increases in spending, the third column lists the forecasted student enrollments.
```{r}
# Import and clean data
predData <- read.csv("C:/Users/george/Documents/Digital Marketing Data/Triggers/predictedSpendingAndEnrollments.csv")

# Transform forecast data
dmyPredData <- dummyVars(" ~ .", data = predData, fullRank = T)
predDummy <- data.frame(predict(dmyPredData, newdata = predData))

# Predict forecast data
predEnrollments <- data.frame(predict(modelEnsemble, newdata = predDummy))
predExport <- data.frame(c(predData,predEnrollments))
predExport
#write.csv(predExport, "C:/Users/george/Documents/Digital Marketing Data/Triggers/predictedExport.csv", row.names = FALSE)
```

## Project Limitations

Since this is the first kind of applied data mining project for either company, the transaction channel between company ad spending and student enrollment counts is not yet automated into a database, and it is not clear how either company communicates this type of ROI.  In other words, we can not fully account for the amount of student enrollments that are attributed to ad-generated leads and spending.  Unfortunately, this is a major issue when attempting to forecast future enrollments.  Due to this fact, the model developed appears to be overfit and does not handle new data well.

Future developments may include more grainular, hourly data sourced from the online platforms themselves and perhaps developing a less overfit model.  In addition, maybe a new strategy for seeing ad-generated leads through to enrollments can be optimized for much better business intelligence performance.